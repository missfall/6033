# Rec 13: GFS: Google File System.
### Tues April 4, 2017     

-------------------------------------

## ----- Class Notes ------
The Paper:    
- Written by Google employees
- Published 2003 @ ACM -SOSP (symposium on operating system principles)


- NOTE: GFS allows for concurrent writes to the same file (i.e., writes
from multiple distinct users at the same time).


#### Chunk Sizes:
- 64MB chunk sizes on chunk servers.
    - UNIX uses 512KB chunk sizes
- Since writes are sequential/append , larger chunk sizes have larger locality so you have to interact with the server less.
    - less network overhead because better locality
- Systems have larger files
- Reduces metadata (system has to store smaller map of chunks )
## ------- Reading Questions ----

- What assumptions does GFS rely on?
    - First, one key assumption is that the filesystem has a modest amount of large files. Whereas most file systems use 512KB sectors, the GFS uses 64MB _chunks_. Upon numerous other assumptions, the assumption that files are seldom mutated again after there initial writing is important to the system design.

- How does it exploit those assumptions?
    - The large file assumption improves performance by minimizing the amount of communication with the Master, and by keeping persistent TCP connections to a single chunk. The assumption that files are seldom written to again after their initial writing is also important to the system design. As files are replicated and stored on 3+ chunk servers, the system is not optimized to handle update all of these replica chunks.

- Why does GFS make those assumptions?
    - GFS makes these assumptions because they hold up in practice for the majority of operations. Rather than setting absolutes, they generalized their use cases and built the system based on the generalization, rather than making sure all edge cases are covered. In practice, this more amortized design performs better.
